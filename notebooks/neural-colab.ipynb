{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a376e7c3-e8ab-467a-a3e4-5af9d0a9ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0b34d0a-5f77-4b30-8fdc-12aaff3008d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('../data/movielens/u.data', sep='\\t', names=column_names)\n",
    "\n",
    "# Load movie titles\n",
    "movie_titles = pd.read_csv('../data/movielens/u.item', sep='|', encoding='latin-1',\n",
    "                           usecols=[0, 1], names=['item_id', 'title'])\n",
    "\n",
    "# Merge the datasets\n",
    "data = pd.merge(ratings, movie_titles, on='item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d055b501-ccf9-421d-82d8-f7e68e472a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['interaction'] = data['rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10eab781-a473-437d-8922-7502d3b0456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = data['user_id'].unique().tolist()\n",
    "item_ids = data['item_id'].unique().tolist()\n",
    "\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "item_id_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "data['user_idx'] = data['user_id'].map(user_id_to_idx)\n",
    "data['item_idx'] = data['item_id'].map(item_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3f5919a-eb84-45ec-8298-865b59e55843",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data[['user_idx', 'item_idx', 'interaction']], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87262bfc-0cc7-4f77-8796-0c5949e62ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_size = 32  # Adjustable based on experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32d1ea69-0327-41b0-bf7a-5c56c02d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input and embedding\n",
    "user_input = keras.Input(shape=(1,), name='user_input')\n",
    "user_embedding = layers.Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n",
    "user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "# Item input and embedding\n",
    "item_input = keras.Input(shape=(1,), name='item_input')\n",
    "item_embedding = layers.Embedding(num_items, embedding_size, name='item_embedding')(item_input)\n",
    "item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "# Concatenate user and item embeddings\n",
    "concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "# MLP layers\n",
    "dense = layers.Dense(128, activation='relu')(concat)\n",
    "dense = layers.Dense(64, activation='relu')(dense)\n",
    "dense = layers.Dense(32, activation='relu')(dense)\n",
    "\n",
    "# Output layer\n",
    "output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# Define the model\n",
    "ncf_model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f454225-0462-44ba-aeaa-a38cf38e5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    # User input and embedding\n",
    "    user_input = keras.Input(shape=(1,), name='user_input')\n",
    "    user_embedding = layers.Embedding(num_users, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(user_input)\n",
    "    user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "    # Item input and embedding\n",
    "    item_input = keras.Input(shape=(1,), name='item_input')\n",
    "    item_embedding = layers.Embedding(num_items, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(item_input)\n",
    "    item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate user and item embeddings\n",
    "    concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "    # MLP layers with hyperparameters for dense units and dropout rate\n",
    "    dense = layers.Dense(hp.Int('units_1', min_value=64, max_value=256, step=64), activation='relu')(concat)\n",
    "    dense = layers.Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1))(dense)\n",
    "    dense = layers.Dense(hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu')(dense)\n",
    "    dense = layers.Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1))(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca49e72-3349-49ce-ada5-fa212d7b82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for training and testing\n",
    "train_user = train_data['user_idx'].values\n",
    "train_item = train_data['item_idx'].values\n",
    "train_label = train_data['interaction'].values\n",
    "\n",
    "test_user = test_data['user_idx'].values\n",
    "test_item = test_data['item_idx'].values\n",
    "test_label = test_data['interaction'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cfd8075-e984-4301-ba05-73cf92fc1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir\\movie_recommender\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='movie_recommender'\n",
    ")\n",
    "\n",
    "# Run the tuner search\n",
    "tuner.search(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    batch_size=256,\n",
    "    epochs=20,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "341b06d2-5ab7-4e82-986d-96f8903a0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['user_input', 'item_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7448 - loss: 0.5125 - val_accuracy: 0.7117 - val_loss: 0.5596\n",
      "Epoch 2/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7498 - loss: 0.5042 - val_accuracy: 0.7125 - val_loss: 0.5656\n",
      "Epoch 3/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7552 - loss: 0.4981 - val_accuracy: 0.7130 - val_loss: 0.5659\n",
      "Epoch 4/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7604 - loss: 0.4898 - val_accuracy: 0.7122 - val_loss: 0.5694\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model from the tuner\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model with early stopping\n",
    "history = best_model.fit(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3e105f6-0100-47c4-8203-93db212d65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.7110 - loss: 0.5700\n",
      "Test Loss: 0.5694, Test Accuracy: 0.7122\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model.evaluate([test_user, test_item], test_label)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6e52a58-ac54-4c34-94fe-20fa4e38911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] >= 4)]\n",
    "    liked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return liked_movies\n",
    "\n",
    "def get_disliked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] <= 2)]\n",
    "    disliked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return disliked_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "502c81be-67e3-4cfc-bfc4-e7a6b5c12314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    user_idx = user_id_to_idx.get(user_id)\n",
    "    if user_idx is None:\n",
    "        print(\"User ID not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Items the user has interacted with\n",
    "    user_data = data[data['user_idx'] == user_idx]\n",
    "    interacted_items = set(user_data['item_idx'].tolist())\n",
    "    \n",
    "    # Items not yet interacted with\n",
    "    all_items = set(range(num_items))\n",
    "    items_to_predict = list(all_items - interacted_items)\n",
    "    \n",
    "    # Predict interaction scores\n",
    "    user_array = np.full(len(items_to_predict), user_idx)\n",
    "    item_array = np.array(items_to_predict)\n",
    "    \n",
    "    predictions = best_model.predict([user_array, item_array], batch_size=1024).flatten()\n",
    "    \n",
    "    # Get top N items\n",
    "    top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "    recommended_item_idxs = [items_to_predict[i] for i in top_indices]\n",
    "    \n",
    "    # Map item indices to titles\n",
    "    recommended_item_ids = [item_ids[idx] for idx in recommended_item_idxs]\n",
    "    recommended_titles = movie_titles[movie_titles['item_id'].isin(recommended_item_ids)]['title'].tolist()\n",
    "    \n",
    "    return recommended_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "878d2718-b577-4042-9e27-81d11a197ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Final Report for User 579:\n",
      "\n",
      "Movies They Liked:\n",
      "1. English Patient, The (1996)\n",
      "2. That Thing You Do! (1996)\n",
      "3. Fish Called Wanda, A (1988)\n",
      "4. Dave (1993)\n",
      "5. Wrong Trousers, The (1993)\n",
      "6. Birdcage, The (1996)\n",
      "7. Strictly Ballroom (1992)\n",
      "8. Silence of the Lambs, The (1991)\n",
      "9. Fargo (1996)\n",
      "10. Sting, The (1973)\n",
      "\n",
      "Movies They Didn't Like:\n",
      "1. Excess Baggage (1997)\n",
      "2. Devil's Own, The (1997)\n",
      "3. Get Shorty (1995)\n",
      "4. Forrest Gump (1994)\n",
      "5. Bye Bye, Love (1995)\n",
      "6. Evita (1996)\n",
      "7. Tank Girl (1995)\n",
      "\n",
      "Recommended Movies They Might Like:\n",
      "1. Loch Ness (1995)\n",
      "2. Whole Wide World, The (1996)\n",
      "3. Some Folks Call It a Sling Blade (1993)\n",
      "4. Innocents, The (1961)\n",
      "5. Fresh (1994)\n",
      "6. Denise Calls Up (1995)\n",
      "7. Stonewall (1995)\n",
      "8. Pather Panchali (1955)\n",
      "9. Santa with Muscles (1996)\n",
      "10. Bitter Sugar (Azucar Amargo) (1996)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Choose a random user ID from the dataset\n",
    "random_user_id = random.choice(user_ids)\n",
    "\n",
    "# Get liked, disliked, and recommended movies\n",
    "liked_movies = get_liked_movies(random_user_id, num_movies=10)\n",
    "disliked_movies = get_disliked_movies(random_user_id, num_movies=10)\n",
    "recommended_movies = recommend_movies(random_user_id, num_recommendations=10)\n",
    "\n",
    "# Display the final report\n",
    "print(f\"Final Report for User {random_user_id}:\")\n",
    "print(\"\\nMovies They Liked:\")\n",
    "for idx, title in enumerate(liked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nMovies They Didn't Like:\")\n",
    "for idx, title in enumerate(disliked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nRecommended Movies They Might Like:\")\n",
    "for idx, title in enumerate(recommended_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c360bcd-b396-4f6c-ab3c-4b16c313fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_model.save('../app/models/ncf_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "490fbdbd-1bda-494a-a27d-8555b2cef058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save user_id_to_idx mapping\n",
    "with open('../app/models/user_id_to_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_to_idx, f)\n",
    "\n",
    "# Save item_ids list (index corresponds to item_idx)\n",
    "with open('../app/models/item_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(item_ids, f)\n",
    "\n",
    "# Save item_id_to_title mapping\n",
    "item_id_to_title = dict(zip(movie_titles['item_id'], movie_titles['title']))\n",
    "with open('../app/models/item_id_to_title.pkl', 'wb') as f:\n",
    "    pickle.dump(item_id_to_title, f)\n",
    "\n",
    "# Save data DataFrame (if needed for liked/disliked movies)\n",
    "data.to_pickle('../app/models/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2293067d-d1d4-430f-af7d-cc6c257ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of user IDs\n",
    "with open('../app/models/user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(user_ids, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc5f18-037e-4e20-a7ca-0cfee0f7b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
