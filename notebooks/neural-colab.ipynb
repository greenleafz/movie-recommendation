{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a376e7c3-e8ab-467a-a3e4-5af9d0a9ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b34d0a-5f77-4b30-8fdc-12aaff3008d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('../data/movielens/u.data', sep='\\t', names=column_names)\n",
    "\n",
    "# Load movie titles\n",
    "movie_titles = pd.read_csv('../data/movielens/u.item', sep='|', encoding='latin-1',\n",
    "                           usecols=[0, 1], names=['item_id', 'title'])\n",
    "\n",
    "# Merge the datasets\n",
    "data = pd.merge(ratings, movie_titles, on='item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d055b501-ccf9-421d-82d8-f7e68e472a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['interaction'] = data['rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10eab781-a473-437d-8922-7502d3b0456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = data['user_id'].unique().tolist()\n",
    "item_ids = data['item_id'].unique().tolist()\n",
    "\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "item_id_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "data['user_idx'] = data['user_id'].map(user_id_to_idx)\n",
    "data['item_idx'] = data['item_id'].map(item_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f5919a-eb84-45ec-8298-865b59e55843",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data[['user_idx', 'item_idx', 'interaction']], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87262bfc-0cc7-4f77-8796-0c5949e62ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_size = 32  # Adjustable based on experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d1ea69-0327-41b0-bf7a-5c56c02d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input and embedding\n",
    "user_input = keras.Input(shape=(1,), name='user_input')\n",
    "user_embedding = layers.Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n",
    "user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "# Item input and embedding\n",
    "item_input = keras.Input(shape=(1,), name='item_input')\n",
    "item_embedding = layers.Embedding(num_items, embedding_size, name='item_embedding')(item_input)\n",
    "item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "# Concatenate user and item embeddings\n",
    "concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "# MLP layers\n",
    "dense = layers.Dense(128, activation='relu')(concat)\n",
    "dense = layers.Dense(64, activation='relu')(dense)\n",
    "dense = layers.Dense(32, activation='relu')(dense)\n",
    "\n",
    "# Output layer\n",
    "output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# Define the model\n",
    "ncf_model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac186c67-db87-45fb-ba5e-8cb00479b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_samples(df, num_negatives=4):\n",
    "    import random\n",
    "    users, items, labels = [], [], []\n",
    "    user_item_set = set(zip(df['user_idx'], df['item_idx']))\n",
    "    all_items = set(range(num_items))\n",
    "    for (u, i) in user_item_set:\n",
    "        users.append(u)\n",
    "        items.append(i)\n",
    "        labels.append(1)\n",
    "        for _ in range(num_negatives):\n",
    "            negative_item = random.choice(list(all_items - set([i])))\n",
    "            users.append(u)\n",
    "            items.append(negative_item)\n",
    "            labels.append(0)\n",
    "    return pd.DataFrame({'user_idx': users, 'item_idx': items, 'interaction': labels})\n",
    "\n",
    "# Create new train and test datasets with negative samples\n",
    "train_data_neg = create_negative_samples(train_data, num_negatives=4)\n",
    "test_data_neg = create_negative_samples(test_data, num_negatives=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f454225-0462-44ba-aeaa-a38cf38e5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    # User input and embedding\n",
    "    user_input = keras.Input(shape=(1,), name='user_input')\n",
    "    user_embedding = layers.Embedding(num_users, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(user_input)\n",
    "    user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "    # Item input and embedding\n",
    "    item_input = keras.Input(shape=(1,), name='item_input')\n",
    "    item_embedding = layers.Embedding(num_items, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(item_input)\n",
    "    item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate user and item embeddings\n",
    "    concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "    # MLP layers with hyperparameters for dense units and dropout rate\n",
    "    dense = layers.Dense(hp.Int('units_1', min_value=64, max_value=256, step=64), activation='relu')(concat)\n",
    "    dense = layers.Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1))(dense)\n",
    "    dense = layers.Dense(hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu')(dense)\n",
    "    dense = layers.Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1))(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca49e72-3349-49ce-ada5-fa212d7b82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for training and testing\n",
    "train_user = train_data['user_idx'].values\n",
    "train_item = train_data['item_idx'].values\n",
    "train_label = train_data['interaction'].values\n",
    "\n",
    "test_user = test_data['user_idx'].values\n",
    "test_item = test_data['item_idx'].values\n",
    "test_label = test_data['interaction'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfd8075-e984-4301-ba05-73cf92fc1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir\\movie_recommender\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='movie_recommender'\n",
    ")\n",
    "\n",
    "# Run the tuner search\n",
    "tuner.search(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    batch_size=256,\n",
    "    epochs=20,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341b06d2-5ab7-4e82-986d-96f8903a0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['user_input', 'item_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7428 - loss: 0.5142 - val_accuracy: 0.7096 - val_loss: 0.5614\n",
      "Epoch 2/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7505 - loss: 0.5058 - val_accuracy: 0.7128 - val_loss: 0.5628\n",
      "Epoch 3/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7585 - loss: 0.4946 - val_accuracy: 0.7127 - val_loss: 0.5629\n",
      "Epoch 4/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7608 - loss: 0.4906 - val_accuracy: 0.7113 - val_loss: 0.5668\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model from the tuner\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model with early stopping\n",
    "history = best_model.fit(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e105f6-0100-47c4-8203-93db212d65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.7102 - loss: 0.5662\n",
      "Test Loss: 0.5668, Test Accuracy: 0.7113\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model.evaluate([test_user, test_item], test_label)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e52a58-ac54-4c34-94fe-20fa4e38911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] >= 4)]\n",
    "    liked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return liked_movies\n",
    "\n",
    "def get_disliked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] <= 2)]\n",
    "    disliked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return disliked_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "502c81be-67e3-4cfc-bfc4-e7a6b5c12314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    user_idx = user_id_to_idx.get(user_id)\n",
    "    if user_idx is None:\n",
    "        print(\"User ID not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Items the user has interacted with\n",
    "    user_data = data[data['user_idx'] == user_idx]\n",
    "    interacted_items = set(user_data['item_idx'].tolist())\n",
    "    \n",
    "    # Items not yet interacted with\n",
    "    all_items = set(range(num_items))\n",
    "    items_to_predict = list(all_items - interacted_items)\n",
    "    \n",
    "    # Predict interaction scores\n",
    "    user_array = np.full(len(items_to_predict), user_idx)\n",
    "    item_array = np.array(items_to_predict)\n",
    "    \n",
    "    predictions = best_model.predict([user_array, item_array], batch_size=1024).flatten()\n",
    "    \n",
    "    # Get top N items\n",
    "    top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "    recommended_item_idxs = [items_to_predict[i] for i in top_indices]\n",
    "    \n",
    "    # Map item indices to titles\n",
    "    recommended_item_ids = [item_ids[idx] for idx in recommended_item_idxs]\n",
    "    recommended_titles = movie_titles[movie_titles['item_id'].isin(recommended_item_ids)]['title'].tolist()\n",
    "    \n",
    "    return recommended_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "878d2718-b577-4042-9e27-81d11a197ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Final Report for User 376:\n",
      "\n",
      "Movies They Liked:\n",
      "1. Graduate, The (1967)\n",
      "2. Full Monty, The (1997)\n",
      "3. One Flew Over the Cuckoo's Nest (1975)\n",
      "4. Seven (Se7en) (1995)\n",
      "5. Monty Python's Life of Brian (1979)\n",
      "6. Sense and Sensibility (1995)\n",
      "7. Rear Window (1954)\n",
      "8. Sling Blade (1996)\n",
      "9. Enchanted April (1991)\n",
      "10. Beautiful Girls (1996)\n",
      "\n",
      "Movies They Didn't Like:\n",
      "\n",
      "Recommended Movies They Might Like:\n",
      "1. Whole Wide World, The (1996)\n",
      "2. Crossfire (1947)\n",
      "3. When We Were Kings (1996)\n",
      "4. Fresh (1994)\n",
      "5. Faust (1994)\n",
      "6. Mina Tannenbaum (1994)\n",
      "7. Anna (1996)\n",
      "8. Pather Panchali (1955)\n",
      "9. Cérémonie, La (1995)\n",
      "10. Bitter Sugar (Azucar Amargo) (1996)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Choose a random user ID from the dataset\n",
    "random_user_id = random.choice(user_ids)\n",
    "\n",
    "# Get liked, disliked, and recommended movies\n",
    "liked_movies = get_liked_movies(random_user_id, num_movies=10)\n",
    "disliked_movies = get_disliked_movies(random_user_id, num_movies=10)\n",
    "recommended_movies = recommend_movies(random_user_id, num_recommendations=10)\n",
    "\n",
    "# Display the final report\n",
    "print(f\"Final Report for User {random_user_id}:\")\n",
    "print(\"\\nMovies They Liked:\")\n",
    "for idx, title in enumerate(liked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nMovies They Didn't Like:\")\n",
    "for idx, title in enumerate(disliked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nRecommended Movies They Might Like:\")\n",
    "for idx, title in enumerate(recommended_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c360bcd-b396-4f6c-ab3c-4b16c313fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_model.save('../app/models/ncf_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490fbdbd-1bda-494a-a27d-8555b2cef058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save user_id_to_idx mapping\n",
    "with open('../app/models/user_id_to_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_to_idx, f)\n",
    "\n",
    "# Save item_ids list (index corresponds to item_idx)\n",
    "with open('../app/models/item_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(item_ids, f)\n",
    "\n",
    "# Save item_id_to_title mapping\n",
    "item_id_to_title = dict(zip(movie_titles['item_id'], movie_titles['title']))\n",
    "with open('../app/models/item_id_to_title.pkl', 'wb') as f:\n",
    "    pickle.dump(item_id_to_title, f)\n",
    "\n",
    "# Save data DataFrame (if needed for liked/disliked movies)\n",
    "data.to_pickle('../app/models/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2293067d-d1d4-430f-af7d-cc6c257ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of user IDs\n",
    "with open('../app/models/user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(user_ids, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df574e55-72c8-49e7-82cd-a010941f25da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input\n",
      "item_input\n",
      "embedding\n",
      "embedding_1\n",
      "flatten\n",
      "flatten_1\n",
      "concatenate\n",
      "dense\n",
      "dropout\n",
      "dense_1\n",
      "dropout_1\n",
      "dense_2\n"
     ]
    }
   ],
   "source": [
    "# Print all layer names in the ncf_model to verify available layers\n",
    "for layer in ncf_model.layers:\n",
    "    print(layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50100116-1ccf-4307-ae93-a566e772e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data for 'Inception (2010)': {'page': 1, 'results': [], 'total_pages': 1, 'total_results': 0}\n",
      "Title: Inception (2010), Poster URL: No poster found or error in response\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Set up the API URL and API key (replace with your actual API key)\n",
    "API_KEY = os.getenv(\"MOVIE_API_KEY\", \"16138e16f189133c632250017548d0db\")\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "# Define a function to get movie poster URLs for a given movie title\n",
    "def get_movie_data(movie_title):\n",
    "    search_url = f\"{BASE_URL}/search/movie\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"query\": movie_title\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    \n",
    "    try:\n",
    "        data = response.json()\n",
    "        # Print the data to inspect the structure\n",
    "        print(f\"Response data for '{movie_title}':\", data)\n",
    "\n",
    "        # Check if any results were returned\n",
    "        if \"results\" in data and data[\"results\"]:\n",
    "            movie_info = data[\"results\"][0]  # Take the first result\n",
    "            poster_path = movie_info.get(\"poster_path\")\n",
    "            if poster_path:\n",
    "                poster_url = f\"https://image.tmdb.org/t/p/w500{poster_path}\"\n",
    "            else:\n",
    "                poster_url = \"No poster found\"\n",
    "            return {\"title\": movie_info[\"title\"], \"poster_url\": poster_url}\n",
    "        else:\n",
    "            return {\"title\": movie_title, \"poster_url\": \"No poster found or error in response\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing movie '{movie_title}':\", e)\n",
    "        return {\"title\": movie_title, \"poster_url\": \"Error occurred\"}\n",
    "\n",
    "# Test with a sample list of movie titles\n",
    "movie_titles = [\"Inception (2010)\"]\n",
    "movie_data = [get_movie_data(title) for title in movie_titles]\n",
    "\n",
    "# Display the results\n",
    "for movie in movie_data:\n",
    "    print(f\"Title: {movie['title']}, Poster URL: {movie['poster_url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33a2146b-2bbf-4348-94c9-92afff787f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\green\\anaconda3\\envs\\neuralcollab\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\green\\anaconda3\\envs\\neuralcollab\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\green\\anaconda3\\envs\\neuralcollab\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\green\\anaconda3\\envs\\neuralcollab\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\green\\anaconda3\\envs\\neuralcollab\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\green\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\green\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\green\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\green\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\green\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fd71f-9e42-4c37-bb0e-8859fb58c706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
