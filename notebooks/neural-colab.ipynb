{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a376e7c3-e8ab-467a-a3e4-5af9d0a9ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b34d0a-5f77-4b30-8fdc-12aaff3008d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('../data/movielens/u.data', sep='\\t', names=column_names)\n",
    "\n",
    "# Load movie titles\n",
    "movie_titles = pd.read_csv('../data/movielens/u.item', sep='|', encoding='latin-1',\n",
    "                           usecols=[0, 1], names=['item_id', 'title'])\n",
    "\n",
    "# Merge the datasets\n",
    "data = pd.merge(ratings, movie_titles, on='item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d055b501-ccf9-421d-82d8-f7e68e472a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['interaction'] = data['rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10eab781-a473-437d-8922-7502d3b0456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = data['user_id'].unique().tolist()\n",
    "item_ids = data['item_id'].unique().tolist()\n",
    "\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "item_id_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "data['user_idx'] = data['user_id'].map(user_id_to_idx)\n",
    "data['item_idx'] = data['item_id'].map(item_id_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3f5919a-eb84-45ec-8298-865b59e55843",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data[['user_idx', 'item_idx', 'interaction']], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87262bfc-0cc7-4f77-8796-0c5949e62ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_size = 32  # Adjustable based on experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d1ea69-0327-41b0-bf7a-5c56c02d88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input and embedding\n",
    "user_input = keras.Input(shape=(1,), name='user_input')\n",
    "user_embedding = layers.Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n",
    "user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "# Item input and embedding\n",
    "item_input = keras.Input(shape=(1,), name='item_input')\n",
    "item_embedding = layers.Embedding(num_items, embedding_size, name='item_embedding')(item_input)\n",
    "item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "# Concatenate user and item embeddings\n",
    "concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "# MLP layers\n",
    "dense = layers.Dense(128, activation='relu')(concat)\n",
    "dense = layers.Dense(64, activation='relu')(dense)\n",
    "dense = layers.Dense(32, activation='relu')(dense)\n",
    "\n",
    "# Output layer\n",
    "output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# Define the model\n",
    "ncf_model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f454225-0462-44ba-aeaa-a38cf38e5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    # User input and embedding\n",
    "    user_input = keras.Input(shape=(1,), name='user_input')\n",
    "    user_embedding = layers.Embedding(num_users, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(user_input)\n",
    "    user_embedding = layers.Flatten()(user_embedding)\n",
    "\n",
    "    # Item input and embedding\n",
    "    item_input = keras.Input(shape=(1,), name='item_input')\n",
    "    item_embedding = layers.Embedding(num_items, hp.Int('embedding_size', min_value=16, max_value=64, step=16))(item_input)\n",
    "    item_embedding = layers.Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate user and item embeddings\n",
    "    concat = layers.Concatenate()([user_embedding, item_embedding])\n",
    "\n",
    "    # MLP layers with hyperparameters for dense units and dropout rate\n",
    "    dense = layers.Dense(hp.Int('units_1', min_value=64, max_value=256, step=64), activation='relu')(concat)\n",
    "    dense = layers.Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1))(dense)\n",
    "    dense = layers.Dense(hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu')(dense)\n",
    "    dense = layers.Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1))(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = keras.Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cfd8075-e984-4301-ba05-73cf92fc1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 04s]\n",
      "val_accuracy: 0.7127500176429749\n",
      "\n",
      "Best val_accuracy So Far: 0.7127500176429749\n",
      "Total elapsed time: 00h 00m 54s\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='movie_recommender'\n",
    ")\n",
    "\n",
    "# Run the tuner search\n",
    "tuner.search(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    batch_size=256,\n",
    "    epochs=20,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "341b06d2-5ab7-4e82-986d-96f8903a0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\anaconda3\\envs\\NeuralCollab\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7435 - loss: 0.5113 - val_accuracy: 0.7136 - val_loss: 0.5596\n",
      "Epoch 2/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7482 - loss: 0.5065 - val_accuracy: 0.7111 - val_loss: 0.5616\n",
      "Epoch 3/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7551 - loss: 0.4969 - val_accuracy: 0.7142 - val_loss: 0.5642\n",
      "Epoch 4/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7617 - loss: 0.4876 - val_accuracy: 0.7110 - val_loss: 0.5713\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model from the tuner\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best model with early stopping\n",
    "history = best_model.fit(\n",
    "    [train_user, train_item],\n",
    "    train_label,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    validation_data=([test_user, test_item], test_label),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e105f6-0100-47c4-8203-93db212d65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 0.7107 - loss: 0.5695\n",
      "Test Loss: 0.5713, Test Accuracy: 0.7110\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = best_model.evaluate([test_user, test_item], test_label)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6e52a58-ac54-4c34-94fe-20fa4e38911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] >= 4)]\n",
    "    liked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return liked_movies\n",
    "\n",
    "def get_disliked_movies(user_id, num_movies=10):\n",
    "    user_data = data[(data['user_id'] == user_id) & (data['rating'] <= 2)]\n",
    "    disliked_movies = user_data.sample(n=min(num_movies, len(user_data)))['title'].tolist()\n",
    "    return disliked_movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "502c81be-67e3-4cfc-bfc4-e7a6b5c12314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    user_idx = user_id_to_idx.get(user_id)\n",
    "    if user_idx is None:\n",
    "        print(\"User ID not found.\")\n",
    "        return []\n",
    "    \n",
    "    # Items the user has interacted with\n",
    "    user_data = data[data['user_idx'] == user_idx]\n",
    "    interacted_items = set(user_data['item_idx'].tolist())\n",
    "    \n",
    "    # Items not yet interacted with\n",
    "    all_items = set(range(num_items))\n",
    "    items_to_predict = list(all_items - interacted_items)\n",
    "    \n",
    "    # Predict interaction scores\n",
    "    user_array = np.full(len(items_to_predict), user_idx)\n",
    "    item_array = np.array(items_to_predict)\n",
    "    \n",
    "    predictions = best_model.predict([user_array, item_array], batch_size=1024).flatten()\n",
    "    \n",
    "    # Get top N items\n",
    "    top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "    recommended_item_idxs = [items_to_predict[i] for i in top_indices]\n",
    "    \n",
    "    # Map item indices to titles\n",
    "    recommended_item_ids = [item_ids[idx] for idx in recommended_item_idxs]\n",
    "    recommended_titles = movie_titles[movie_titles['item_id'].isin(recommended_item_ids)]['title'].tolist()\n",
    "    \n",
    "    return recommended_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "878d2718-b577-4042-9e27-81d11a197ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Final Report for User 306:\n",
      "\n",
      "Movies They Liked:\n",
      "1. Marvin's Room (1996)\n",
      "2. Michael Collins (1996)\n",
      "3. Cold Comfort Farm (1995)\n",
      "4. Stealing Beauty (1996)\n",
      "5. English Patient, The (1996)\n",
      "6. Kolya (1996)\n",
      "7. Fargo (1996)\n",
      "8. Secrets & Lies (1996)\n",
      "9. Mighty Aphrodite (1995)\n",
      "10. Antonia's Line (1995)\n",
      "\n",
      "Movies They Didn't Like:\n",
      "1. Contact (1997)\n",
      "2. Last Supper, The (1995)\n",
      "3. Grumpier Old Men (1995)\n",
      "\n",
      "Recommended Movies They Might Like:\n",
      "1. Whole Wide World, The (1996)\n",
      "2. Innocents, The (1961)\n",
      "3. When We Were Kings (1996)\n",
      "4. Fresh (1994)\n",
      "5. Maybe, Maybe Not (Bewegte Mann, Der) (1994)\n",
      "6. Faust (1994)\n",
      "7. Mina Tannenbaum (1994)\n",
      "8. Stonewall (1995)\n",
      "9. Pather Panchali (1955)\n",
      "10. Bitter Sugar (Azucar Amargo) (1996)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Choose a random user ID from the dataset\n",
    "random_user_id = random.choice(user_ids)\n",
    "\n",
    "# Get liked, disliked, and recommended movies\n",
    "liked_movies = get_liked_movies(random_user_id, num_movies=10)\n",
    "disliked_movies = get_disliked_movies(random_user_id, num_movies=10)\n",
    "recommended_movies = recommend_movies(random_user_id, num_recommendations=10)\n",
    "\n",
    "# Display the final report\n",
    "print(f\"Final Report for User {random_user_id}:\")\n",
    "print(\"\\nMovies They Liked:\")\n",
    "for idx, title in enumerate(liked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nMovies They Didn't Like:\")\n",
    "for idx, title in enumerate(disliked_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n",
    "print(\"\\nRecommended Movies They Might Like:\")\n",
    "for idx, title in enumerate(recommended_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c360bcd-b396-4f6c-ab3c-4b16c313fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_model.save('../app/models/ncf_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "490fbdbd-1bda-494a-a27d-8555b2cef058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save user_id_to_idx mapping\n",
    "with open('../app/models/user_id_to_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_to_idx, f)\n",
    "\n",
    "# Save item_ids list (index corresponds to item_idx)\n",
    "with open('../app/models/item_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(item_ids, f)\n",
    "\n",
    "# Save item_id_to_title mapping\n",
    "item_id_to_title = dict(zip(movie_titles['item_id'], movie_titles['title']))\n",
    "with open('../app/models/item_id_to_title.pkl', 'wb') as f:\n",
    "    pickle.dump(item_id_to_title, f)\n",
    "\n",
    "# Save data DataFrame (if needed for liked/disliked movies)\n",
    "data.to_pickle('../app/models/data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2293067d-d1d4-430f-af7d-cc6c257ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of user IDs\n",
    "with open('../app/models/user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(user_ids, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc5f18-037e-4e20-a7ca-0cfee0f7b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
